{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Joebaho Cloud Platform Website : www.joebahocloud.com Email: joebachou@gmail.com Phone: +1-408-449-9131 WhatsApp: +1-408-449-9131 Linkedin: https://www.linkedin.com/in/josephmbatchou Twitter: https://twitter.com/Joebaho237 Github: https://github.com/Joebaho Dockerhub: https://hub.docker.com/u/joebaho2 Terraform: https://app.terraform.io/app/settings/profile Welcome to Joebaho Cloud Platform ,, Your Gateway to the Cloud At Joebaho Cloud Platform ,, we understand that the cloud isn't just a technology\u2014it's a strategic imperative. It's the key to agility, resilience, and competitive advantage in an ever-evolving market. That's why we're dedicated to providing you with cutting-edge cloud services that deliver unparalleled performance, security, and flexibility. At Joebaho Cloud Platform , our main goal here is to create a platform where we will be presenting some projects. Joebaho Cloud Platform is dedicated to providing you with the necessary support and tools to improve knowledges in the cloud environment. Understanding the process, improve status and getting the dream job are the top priorities here. I understand it can be difficult to study alone once the training, the BootCamp, the study or personal reasearch is over. Here is a dedicated place to place or keep our learning process in the cloud. Explore our website to learn more about our cloud offerings, customer success stories, and industry insights. Ready to embark on your cloud journey? Get in touch with us today, and let's elevate your knowledge to new heights together. Welcome to the future of computing. Welcome to Joebaho Cloud Platform ,.","title":"Home"},{"location":"#welcome-to-joebaho-cloud-platform","text":"Website : www.joebahocloud.com Email: joebachou@gmail.com Phone: +1-408-449-9131 WhatsApp: +1-408-449-9131 Linkedin: https://www.linkedin.com/in/josephmbatchou Twitter: https://twitter.com/Joebaho237 Github: https://github.com/Joebaho Dockerhub: https://hub.docker.com/u/joebaho2 Terraform: https://app.terraform.io/app/settings/profile Welcome to Joebaho Cloud Platform ,, Your Gateway to the Cloud At Joebaho Cloud Platform ,, we understand that the cloud isn't just a technology\u2014it's a strategic imperative. It's the key to agility, resilience, and competitive advantage in an ever-evolving market. That's why we're dedicated to providing you with cutting-edge cloud services that deliver unparalleled performance, security, and flexibility. At Joebaho Cloud Platform , our main goal here is to create a platform where we will be presenting some projects. Joebaho Cloud Platform is dedicated to providing you with the necessary support and tools to improve knowledges in the cloud environment. Understanding the process, improve status and getting the dream job are the top priorities here. I understand it can be difficult to study alone once the training, the BootCamp, the study or personal reasearch is over. Here is a dedicated place to place or keep our learning process in the cloud. Explore our website to learn more about our cloud offerings, customer success stories, and industry insights. Ready to embark on your cloud journey? Get in touch with us today, and let's elevate your knowledge to new heights together. Welcome to the future of computing. Welcome to Joebaho Cloud Platform ,.","title":"Welcome to Joebaho Cloud Platform"},{"location":"2-tier-terraform-on-aws/","text":"","title":"2 Tier Architecture on AWS with Terraform"},{"location":"3-tier-terraform-on-aws/","text":"Multi-Tier Architecture on AWS using Terraform \ud83d\ude80 Overview: The Multi-Tier Architecture project on AWS using Terraform aims to create a scalable and resilient infrastructure that leverages the power of Amazon Web Services (AWS) cloud platform. This project utilizes Terraform, an Infrastructure as Code (IaC) tool, to provision and manage the infrastructure components, enabling automation, repeatability, and scalability. The primary objective of this project is to design and deploy a multi-tier architecture on AWS that consists of multiple layers, including presentation, application, and database tiers. Each tier is deployed across multiple Availability Zones (AZs) for high availability and fault tolerance. \ud83d\udd27 Problem Statement Terraform is an IaC software tool that provides a consistent command line interface (CLI) workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. In this specific case you need to create foundation Networking(VPC, Subnets, route table, IGW, NAT Gateway...), virtual machines (EC2 instances), databases (RDS), distribution of traffic (ELB) and Auto-scaling (ASG). Terraform will automatically use the configuration files to provide the infrastructure resources and run application needed. Terraform will use his deployment to provide all AWS needed elements avoiding us to use the console and it will automate the setup, ensuring consistency and reducing human error. \ud83d\udcbd Techonology Stack The architecture consists of the following three tiers: VPC : AWS VPC AutoScaling : AWS ASG Elastic Load Balancer : AWS ELB Database : AWS RDS File Configuration : Terraform \ud83d\udccc Architecture Diagram \ud83c\udf1f Project Requirements Before you get started, make sure you have the following prerequisites in place: Terraform installed on your local machine. AWS IAM credentials configured in your text editor. In this case we will use VSCODE. Git installed on your local machine and Github account set up Github Git for cloning the repository. You must know and understand: High Availability : The architecture is designed for fault tolerance and redundancy. Reason why resources will be deploy across two avaibility zones to ensure resilience to failures. Scalability : Easily scale the web and application tiers to handle varying workloads automaticaly based on demand. Security : Implementing security best practices such as Security groups and network ACLs are configured to ensure a secure and protected environment. You must also know Terraform workflow \ud83d\udccb Table of Contents I - Terraform Configuration files Step 1: Provider Configuration Step 2: Variables Configuration Step 3: VPC Configuration Step 4: web tier Configuration Step 5: Application tier Configuration Step 6: Database tier Configuration Step 7: Output Configuration II - Instructions of Deployment Step 8: Clone Repository Step 9: Initialize Folder Step 10: Format Files Step 11: Validate Files Step 12: Plan Step 13: Apply Step 14: Review of Resources Step 15: Destroy \u2728Terraform Configuration files You need to write different files generating resources Step 1: Provider Configuration Here we declare our cloud provider and we specify the region where we will be launching resources provider Configuration Step 2: Variables Configuration This is where we declare all variables and thier value. It includes Variables : List of element that can vary or change. They can be reuse values throughout our code without repeating ourselves and help make the code dynamic values : values attributed to each variables. secrets : username and Password for the Database Reminder: Never push terraform.tfvars and secrets.tfvars file on Github We have variables Configuration value Configuration Secrets Configuration Step 3: VPC Configuration This is where you create the basement, foundation and networking where all the resources will be launch. It includes VPC, Subnets, IGW, NatGateway, EIP and Route tables VPC Configuration Step 4: Web Tier Configuration The Web Tier is the entry point for incoming user requests. Resources are launched in the public subnets. It typically includes: Web Servers : These run your application code that contains the apache which will deploy the index.html located in the user data. Load Balancer : Distributes traffic across multiple web servers running in the public subnets. Auto Scaling : Automatically adjusts the number of web servers based on traffic. Security Groups : Controls incoming and outgoing traffic from outside to the web servers. Web tier configuration files are : Web ASG Configuration Web ELB Configuration Step 5: Application Tier Configuration The Application Tier hosts the application servers responsible for running business logic and interacting with the database tier. Key components include: Application Servers : These run your application code and can be horizontally scaled. Load Balancer : Distributes traffic to the application servers running in the private subnets. Auto Scaling : Automatically adjusts the number of web servers based on traffic. Security Groups : Controls incoming and outgoing traffic from the web servers to the application servers. Application Tier Configuration files are: App ASG Configuration App ELB Configuration Step 6: Database Tier Configuration The Database Tier stores and manages our application data. We use Amazon RDS for a managed, a highly available and scalable database to store application data. Key components include: Subnets groups : List of subnets wherether Server databases will run. Amazon RDS : A managed database service for MySQL/PostgreSQL/SQL Server databases. Security Groups : Control incoming and outgoing traffic to the database. Database Tier Configuration file: DB Configuration Step 7: Output Configuration Know as Output Value : it is a convenient way to get useful information about your infranstructure printed on the CLI. It is showing the ARN, name or ID of a resource. In this case we are bringing out the DNS name of the web application Load balancer. Output Configuration \ud83d\udcbc Instructions of Deployment Follow these steps to deploy the architecture: Step 8: Clone Repository: Clone the repository in your local machine using the command \"git clone\" bash git clone https://github.com/cloudspaceacademy/terraform-on-aws.git Step 9: Initialize Folder Initialize the folder containing configuation files that were clone to Terraform and apply the configuration by typing the following command bash terraform init You must see this image Step 10: Format Files Apply any changes on files and Review the changes and confirm the good format with command: bash terraform fmt Step 11: Validate Files Ensure that every files are syntactically valid and ready to go with the command: bash terraform validate If everything is good you will have something like this Step 12: Plan Create an excution plan to provide the achievement of the desired state. It Check and confirm the numbers of resources that will be create. Use command: bash terraform plan The list of all resources in stage of creation will appear and you can see all properties(arguments and attributs) of each resouces Step 13: Apply Bring all desired state resources on life. It Launch and create all resources listed in the configuration files. The command to perform the task is: bash terraform apply -auto-approve You will be prompt to type the username and password for the database. After you enter those criticals data the process of creation will start and you will be able to see which resourse is on the way to be create and the time it taking to create. At the end you will recieve a prompt message showing all resources status: created, changed and the numbers of them. Step 14: Review of resources Go back on the console and check all actual state resources one by one to see. You will have VPC Instances running Application Load Balancer Autoscaling groups Database Web page Step 15: Destroy Destroy the terraform managed infrastructure meaning all resourcescreated will be shut down. This action can be done with the command \"terraform destroy\" bash terraform destroy -auto-approve At the end you will recieve a prompt message showing all resources has been destroyed \ud83d\udcc4 License This project is licensed under the CloudSpace Academy License","title":"3 Tier Architecture on AWS with Terraform"},{"location":"3-tier-terraform-on-aws/#multi-tier-architecture-on-aws-using-terraform","text":"","title":"Multi-Tier Architecture on AWS using Terraform"},{"location":"3-tier-terraform-on-aws/#overview","text":"The Multi-Tier Architecture project on AWS using Terraform aims to create a scalable and resilient infrastructure that leverages the power of Amazon Web Services (AWS) cloud platform. This project utilizes Terraform, an Infrastructure as Code (IaC) tool, to provision and manage the infrastructure components, enabling automation, repeatability, and scalability. The primary objective of this project is to design and deploy a multi-tier architecture on AWS that consists of multiple layers, including presentation, application, and database tiers. Each tier is deployed across multiple Availability Zones (AZs) for high availability and fault tolerance.","title":"\ud83d\ude80 Overview:"},{"location":"3-tier-terraform-on-aws/#problem-statement","text":"Terraform is an IaC software tool that provides a consistent command line interface (CLI) workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. In this specific case you need to create foundation Networking(VPC, Subnets, route table, IGW, NAT Gateway...), virtual machines (EC2 instances), databases (RDS), distribution of traffic (ELB) and Auto-scaling (ASG). Terraform will automatically use the configuration files to provide the infrastructure resources and run application needed. Terraform will use his deployment to provide all AWS needed elements avoiding us to use the console and it will automate the setup, ensuring consistency and reducing human error.","title":"\ud83d\udd27 Problem Statement"},{"location":"3-tier-terraform-on-aws/#techonology-stack","text":"The architecture consists of the following three tiers: VPC : AWS VPC AutoScaling : AWS ASG Elastic Load Balancer : AWS ELB Database : AWS RDS File Configuration : Terraform","title":"\ud83d\udcbd Techonology Stack"},{"location":"3-tier-terraform-on-aws/#architecture-diagram","text":"","title":"\ud83d\udccc Architecture Diagram"},{"location":"3-tier-terraform-on-aws/#project-requirements","text":"Before you get started, make sure you have the following prerequisites in place: Terraform installed on your local machine. AWS IAM credentials configured in your text editor. In this case we will use VSCODE. Git installed on your local machine and Github account set up Github Git for cloning the repository. You must know and understand: High Availability : The architecture is designed for fault tolerance and redundancy. Reason why resources will be deploy across two avaibility zones to ensure resilience to failures. Scalability : Easily scale the web and application tiers to handle varying workloads automaticaly based on demand. Security : Implementing security best practices such as Security groups and network ACLs are configured to ensure a secure and protected environment. You must also know Terraform workflow","title":"\ud83c\udf1f Project Requirements"},{"location":"3-tier-terraform-on-aws/#table-of-contents","text":"I - Terraform Configuration files Step 1: Provider Configuration Step 2: Variables Configuration Step 3: VPC Configuration Step 4: web tier Configuration Step 5: Application tier Configuration Step 6: Database tier Configuration Step 7: Output Configuration II - Instructions of Deployment Step 8: Clone Repository Step 9: Initialize Folder Step 10: Format Files Step 11: Validate Files Step 12: Plan Step 13: Apply Step 14: Review of Resources Step 15: Destroy","title":"\ud83d\udccb Table of Contents"},{"location":"3-tier-terraform-on-aws/#terraform-configuration-files","text":"You need to write different files generating resources Step 1: Provider Configuration Here we declare our cloud provider and we specify the region where we will be launching resources provider Configuration Step 2: Variables Configuration This is where we declare all variables and thier value. It includes Variables : List of element that can vary or change. They can be reuse values throughout our code without repeating ourselves and help make the code dynamic values : values attributed to each variables. secrets : username and Password for the Database Reminder: Never push terraform.tfvars and secrets.tfvars file on Github We have variables Configuration value Configuration Secrets Configuration Step 3: VPC Configuration This is where you create the basement, foundation and networking where all the resources will be launch. It includes VPC, Subnets, IGW, NatGateway, EIP and Route tables VPC Configuration Step 4: Web Tier Configuration The Web Tier is the entry point for incoming user requests. Resources are launched in the public subnets. It typically includes: Web Servers : These run your application code that contains the apache which will deploy the index.html located in the user data. Load Balancer : Distributes traffic across multiple web servers running in the public subnets. Auto Scaling : Automatically adjusts the number of web servers based on traffic. Security Groups : Controls incoming and outgoing traffic from outside to the web servers. Web tier configuration files are : Web ASG Configuration Web ELB Configuration Step 5: Application Tier Configuration The Application Tier hosts the application servers responsible for running business logic and interacting with the database tier. Key components include: Application Servers : These run your application code and can be horizontally scaled. Load Balancer : Distributes traffic to the application servers running in the private subnets. Auto Scaling : Automatically adjusts the number of web servers based on traffic. Security Groups : Controls incoming and outgoing traffic from the web servers to the application servers. Application Tier Configuration files are: App ASG Configuration App ELB Configuration Step 6: Database Tier Configuration The Database Tier stores and manages our application data. We use Amazon RDS for a managed, a highly available and scalable database to store application data. Key components include: Subnets groups : List of subnets wherether Server databases will run. Amazon RDS : A managed database service for MySQL/PostgreSQL/SQL Server databases. Security Groups : Control incoming and outgoing traffic to the database. Database Tier Configuration file: DB Configuration Step 7: Output Configuration Know as Output Value : it is a convenient way to get useful information about your infranstructure printed on the CLI. It is showing the ARN, name or ID of a resource. In this case we are bringing out the DNS name of the web application Load balancer. Output Configuration","title":"\u2728Terraform Configuration files"},{"location":"3-tier-terraform-on-aws/#instructions-of-deployment","text":"Follow these steps to deploy the architecture: Step 8: Clone Repository: Clone the repository in your local machine using the command \"git clone\" bash git clone https://github.com/cloudspaceacademy/terraform-on-aws.git Step 9: Initialize Folder Initialize the folder containing configuation files that were clone to Terraform and apply the configuration by typing the following command bash terraform init You must see this image Step 10: Format Files Apply any changes on files and Review the changes and confirm the good format with command: bash terraform fmt Step 11: Validate Files Ensure that every files are syntactically valid and ready to go with the command: bash terraform validate If everything is good you will have something like this Step 12: Plan Create an excution plan to provide the achievement of the desired state. It Check and confirm the numbers of resources that will be create. Use command: bash terraform plan The list of all resources in stage of creation will appear and you can see all properties(arguments and attributs) of each resouces Step 13: Apply Bring all desired state resources on life. It Launch and create all resources listed in the configuration files. The command to perform the task is: bash terraform apply -auto-approve You will be prompt to type the username and password for the database. After you enter those criticals data the process of creation will start and you will be able to see which resourse is on the way to be create and the time it taking to create. At the end you will recieve a prompt message showing all resources status: created, changed and the numbers of them. Step 14: Review of resources Go back on the console and check all actual state resources one by one to see. You will have VPC Instances running Application Load Balancer Autoscaling groups Database Web page Step 15: Destroy Destroy the terraform managed infrastructure meaning all resourcescreated will be shut down. This action can be done with the command \"terraform destroy\" bash terraform destroy -auto-approve At the end you will recieve a prompt message showing all resources has been destroyed","title":"\ud83d\udcbc Instructions of Deployment"},{"location":"3-tier-terraform-on-aws/#license","text":"This project is licensed under the CloudSpace Academy License","title":"\ud83d\udcc4 License"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/","text":"Creating a Kubernetes Cluster with Minikube Introduction Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube. Prerequisites Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that Docker | installation guide Click Here kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine. Kubectl | installation guide Click Here Install Minikube Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube. Minikube | installation guide Click Here Start Minikube Open a terminal and run the following command to start Minikube: minikube start This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster. Verify Cluster Status After Minikube has started, you can check the cluster status using: kubectl cluster-info This command will display information about the cluster, including the Kubernetes master and services. Check Nodes Verify that Minikube has created a node for your cluster: kubectl get nodes This command should show the Minikube node with a status of Ready. Kubernetes Dashboard (Optional) If you want to use the Kubernetes Dashboard, you can start it with: minikube dashboard This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster. Interact with Kubernetes Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl. Example Deployment As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 Apply the configuration: kubectl apply -f nginx-deployment.yaml This will deploy two replicas of the Nginx web server. Conclusion Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.","title":"Creating a Kubernetes Cluster with Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#creating-a-kubernetes-cluster-with-minikube","text":"Introduction Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube. Prerequisites Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that Docker | installation guide Click Here kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine. Kubectl | installation guide Click Here","title":"Creating a Kubernetes Cluster with Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#install-minikube","text":"Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube. Minikube | installation guide Click Here","title":"Install Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#start-minikube","text":"Open a terminal and run the following command to start Minikube: minikube start This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster.","title":"Start Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#verify-cluster-status","text":"After Minikube has started, you can check the cluster status using: kubectl cluster-info This command will display information about the cluster, including the Kubernetes master and services.","title":"Verify Cluster Status"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#check-nodes","text":"Verify that Minikube has created a node for your cluster: kubectl get nodes This command should show the Minikube node with a status of Ready.","title":"Check Nodes"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#kubernetes-dashboard-optional","text":"If you want to use the Kubernetes Dashboard, you can start it with: minikube dashboard This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster.","title":"Kubernetes Dashboard (Optional)"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#interact-with-kubernetes","text":"Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl.","title":"Interact with Kubernetes"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#example-deployment","text":"As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 Apply the configuration: kubectl apply -f nginx-deployment.yaml This will deploy two replicas of the Nginx web server.","title":"Example Deployment"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#conclusion","text":"Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.","title":"Conclusion"},{"location":"about/","text":"Who I Am My name is Joseph Mbatchou, and I am grateful for the opportunity to introduce myself to you. With eight years of experience in the tech industry, I currently serve as a Teacher Assistant in Cloud Consulting at CloudSpace Consulting, while also holding the position of Part-Time Shift Lead Access Control at Digital Realty / Allied Universal. My journey began as a Computer Science Teacher and Computer Technician at Biopharcam, a computer sales company in my home country. Upon relocating to the United States, I joined Allied Universal, gradually advancing from an officer to a shift lead position. In this role, I oversaw various applications for training and operational tasks, collaborating closely with engineers and developers from Digital Realty, a data center provider. Observing these professionals at work sparked my interest in cloud computing, leading me to pursue cloud classes, attend boot camps, and conduct in-depth research across various domains, including training at CloudSpace Academy. This exposure deepened my passion for IT and motivated me to transition into the industry to further my personal growth and contribute to organizational success. As a Cloud Consultant, I have been privileged to collaborate with Cloud Solution Architects and DevOps teams on numerous projects, consistently exceeding customer expectations through our dedication and innovative solutions. My tenure in this role has enriched my skill set and broadened my professional experience. Now, equipped with a wealth of knowledge in cloud computing and DevOps practices, I am eager to apply my expertise to new challenges and opportunities. I am confident in my ability to contribute effectively to your knowledge Thank you for considering my hard work. I look forward to have you on board of the learning process.","title":"About"},{"location":"about/#who-i-am","text":"My name is Joseph Mbatchou, and I am grateful for the opportunity to introduce myself to you. With eight years of experience in the tech industry, I currently serve as a Teacher Assistant in Cloud Consulting at CloudSpace Consulting, while also holding the position of Part-Time Shift Lead Access Control at Digital Realty / Allied Universal. My journey began as a Computer Science Teacher and Computer Technician at Biopharcam, a computer sales company in my home country. Upon relocating to the United States, I joined Allied Universal, gradually advancing from an officer to a shift lead position. In this role, I oversaw various applications for training and operational tasks, collaborating closely with engineers and developers from Digital Realty, a data center provider. Observing these professionals at work sparked my interest in cloud computing, leading me to pursue cloud classes, attend boot camps, and conduct in-depth research across various domains, including training at CloudSpace Academy. This exposure deepened my passion for IT and motivated me to transition into the industry to further my personal growth and contribute to organizational success. As a Cloud Consultant, I have been privileged to collaborate with Cloud Solution Architects and DevOps teams on numerous projects, consistently exceeding customer expectations through our dedication and innovative solutions. My tenure in this role has enriched my skill set and broadened my professional experience. Now, equipped with a wealth of knowledge in cloud computing and DevOps practices, I am eager to apply my expertise to new challenges and opportunities. I am confident in my ability to contribute effectively to your knowledge Thank you for considering my hard work. I look forward to have you on board of the learning process.","title":"Who I Am"},{"location":"ansible-terraform-on-aws/","text":"","title":"Ansible Deployment on AWS"},{"location":"dockerbeginning/","text":"Flask Calculator Web Application Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here Getting Started Open terminal and run this command to clone the repository of Flask Calculator Web Application. Command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. You can simply use this command: cd Flask-Calculator-app && cd cloudspace Now run ls command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates. Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile. Why do we use docker? Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment. Now let's elaborate how we write that Dockerfile to package our application. Dockerfile FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 5000 CMD [\"python\", \"app.py\"] Now just break down all the code and elaborate why they use for FROM python:3.9-slim This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications. WORKDIR /app This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory. COPY requirements.txt . This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app). RUN pip install -r requirements.txt This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container. COPY . . This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder. EXPOSE 5000 This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity. CMD [\"python\", \"app.py\"] This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier. Now lets learn how to run the application by using just two docker commands from your local machine. Running the Application with Docker Building a Docker Image To build a Docker image, you use the docker build command. docker build -t flask-calculator . docker build -t flask-calculator . Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . ) Now break down the code docker build: This is the Docker command for building an image. -t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\" .: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image. Docker image used for : A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers. Running a Docker Container Once we have built our Docker image, we can create and run containers from it using the docker run command. docker run -p 5000:80 -d flask-calculator Now break down the code docker run: This is the Docker command for creating and running a container. -p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000. -d: this defines the container will run in detached mode, which means it runs in the background. flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image. Docker Container used for : A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed. Access the application Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.","title":"Flask Calculator Web Application"},{"location":"dockerbeginning/#flask-calculator-web-application","text":"Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here","title":"Flask Calculator Web Application"},{"location":"dockerbeginning/#getting-started","text":"Open terminal and run this command to clone the repository of Flask Calculator Web Application. Command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. You can simply use this command: cd Flask-Calculator-app && cd cloudspace Now run ls command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates. Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile.","title":"Getting Started"},{"location":"dockerbeginning/#why-do-we-use-docker","text":"Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment. Now let's elaborate how we write that Dockerfile to package our application.","title":"Why do we use docker?"},{"location":"dockerbeginning/#dockerfile","text":"FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 5000 CMD [\"python\", \"app.py\"] Now just break down all the code and elaborate why they use for FROM python:3.9-slim This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications. WORKDIR /app This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory. COPY requirements.txt . This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app). RUN pip install -r requirements.txt This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container. COPY . . This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder. EXPOSE 5000 This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity. CMD [\"python\", \"app.py\"] This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier. Now lets learn how to run the application by using just two docker commands from your local machine.","title":"Dockerfile"},{"location":"dockerbeginning/#running-the-application-with-docker","text":"","title":"Running the Application with Docker"},{"location":"dockerbeginning/#building-a-docker-image","text":"To build a Docker image, you use the docker build command. docker build -t flask-calculator . docker build -t flask-calculator . Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . ) Now break down the code docker build: This is the Docker command for building an image. -t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\" .: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image.","title":"Building a Docker Image"},{"location":"dockerbeginning/#docker-image-used-for","text":"A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers.","title":"Docker image used for :"},{"location":"dockerbeginning/#running-a-docker-container","text":"Once we have built our Docker image, we can create and run containers from it using the docker run command. docker run -p 5000:80 -d flask-calculator Now break down the code docker run: This is the Docker command for creating and running a container. -p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000. -d: this defines the container will run in detached mode, which means it runs in the background. flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image.","title":"Running a Docker Container"},{"location":"dockerbeginning/#docker-container-used-for","text":"A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed.","title":"Docker Container used for :"},{"location":"dockerbeginning/#access-the-application","text":"Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.","title":"Access the application"},{"location":"dockercomposebeginning/","text":"Flask Calculator Web Application Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here Getting Started Open your terminal and clone the Flask Calculator Web Application repository using the following command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Navigate to the \"Flask-Calculator-app\" directory By running this command : cd Flask-Calculator-app Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command: cd cloudspace In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile .Now we are gonna learn how to run this application using docker-compose.yml file . In Previous session \"Docker Beginner\" you have learned about Dockerfile .How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file. docker-compose.yml file used for A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack. Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers. docker-compose.yml version: '3.8' services: flask-calculator: build: context: . dockerfile: Dockerfile ports: - \"8080:80\" Let's Break down the code version: '3.8' :Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8. services :Defines the services that make up the Docker application. flask-calculator :The name of the service. In this case, it's named flask-calculator. build :Specifies how to build the Docker image for the service. context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.) dockerfile: Dockerfile :Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile. ports :Specifies the ports to expose from the container. - \"8080:80\": Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port. Running the Application with Docker Compose To run the application run this command where the docker-compose.yml file is located. Command : docker-compose up -d Break down Codes: docker-compose : The Docker Compose command-line tool. up : This command is used to create and start containers based on the configurations specified in the docker-compose.yml file. -d : Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks. Now Run docker ps command to see the running container id that you have just created. Access the application Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.","title":"Flask Calculator Web Application"},{"location":"dockercomposebeginning/#flask-calculator-web-application","text":"Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here","title":"Flask Calculator Web Application"},{"location":"dockercomposebeginning/#getting-started","text":"Open your terminal and clone the Flask Calculator Web Application repository using the following command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Navigate to the \"Flask-Calculator-app\" directory By running this command : cd Flask-Calculator-app Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command: cd cloudspace In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile .Now we are gonna learn how to run this application using docker-compose.yml file . In Previous session \"Docker Beginner\" you have learned about Dockerfile .How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file.","title":"Getting Started"},{"location":"dockercomposebeginning/#docker-composeyml-file-used-for","text":"A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack. Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers.","title":"docker-compose.yml file used for"},{"location":"dockercomposebeginning/#docker-composeyml","text":"version: '3.8' services: flask-calculator: build: context: . dockerfile: Dockerfile ports: - \"8080:80\" Let's Break down the code version: '3.8' :Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8. services :Defines the services that make up the Docker application. flask-calculator :The name of the service. In this case, it's named flask-calculator. build :Specifies how to build the Docker image for the service. context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.) dockerfile: Dockerfile :Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile. ports :Specifies the ports to expose from the container. - \"8080:80\": Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port.","title":"docker-compose.yml"},{"location":"dockercomposebeginning/#running-the-application-with-docker-compose","text":"To run the application run this command where the docker-compose.yml file is located. Command : docker-compose up -d Break down Codes: docker-compose : The Docker Compose command-line tool. up : This command is used to create and start containers based on the configurations specified in the docker-compose.yml file. -d : Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks. Now Run docker ps command to see the running container id that you have just created.","title":"Running the Application with Docker Compose"},{"location":"dockercomposebeginning/#access-the-application","text":"Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.","title":"Access the application"},{"location":"netflix-ish-deployment/","text":"","title":"Docker Compose Beginner"},{"location":"static-web-on-aws/","text":"Static web site hosting on S3 using Terraform Overview: For this project we will have a static website hosted on S3 and will be utilizing CodePipeline to monitor and automatically deploy changes made from our CodeCommit repository where our index.html is hosted. Then we\u2019ll setup CloudFront as a CDN that will redirect HTTP requests to HTTPS. Problem Statement Techonology Stack Architecture Diagram Project Requirements: Your team has asked you to create a way to automate the deployment of a website. Currently your developers have to go through the process manually to test each new update to their code. You\u2019ll need to provide the static site URL to the developers and also make a modification to the code in the GitHub repo to verify the pipeline is working. Create a new repository in GitHub or CodeCommit and load the attached HTML. Create and configure a S3 bucket to host your static website. Create a CI/CD pipeline using the AWS Codepipeline service . Set your repo as the Source Stage of the Codepipeline that is triggered when an update is made to a GitHub repo. For the deploy stage select your S3 bucket. Deploy the pipeline and verify that you can reach the static website. Make an update to the code in your github to verify that the codepipeline is triggered. This can be as simple as a change to the Readme file because any change to the files should trigger the workflow. Note: you can skip the Build stage for this project. Your app is very popular all around the world but some users are complaining about slow load times in some Regions. You have been asked to add CloudFront as a CDN for your static website. CloudFront should allow caching of your static webpage and only allow HTTPS traffic to your site. Instructions Create New Repository and Clone it. First we need to create a repository. Navigate to GitHub -> Repositories -> Create Repository and give it a name. Use the Clone URL to clone it to your local system. Add your files to your local repository, commit your changes, and push your changes. File has been pushed from our local repo to CodeCommit. Create S3 Bucket Navigate to S3 -> Create Bucket. Uncheck \u201cBlock all Public Access\u201d and acknowledge. Navigate to your bucket -> Properties -> Edit Static website hosting Enable Static website hosting and add your index document Now we need to create a bucket policy. Got to Permissions and edit the bucket policy. The following will allow everyone to access the bucket using the GetObject command, Setup Pipeline. Navigate to CodePipeline -> Create pipeline provide a name and click next. Source Provider = AWS CodeCommit Repository name = \u201cSelect your repo from the list\u201d Branch Name = Master","title":"Static Web Hosting on S3"},{"location":"static-web-on-aws/#static-web-site-hosting-on-s3-using-terraform","text":"","title":"Static web site hosting on S3 using Terraform"},{"location":"static-web-on-aws/#overview","text":"For this project we will have a static website hosted on S3 and will be utilizing CodePipeline to monitor and automatically deploy changes made from our CodeCommit repository where our index.html is hosted. Then we\u2019ll setup CloudFront as a CDN that will redirect HTTP requests to HTTPS.","title":"Overview:"},{"location":"static-web-on-aws/#problem-statement","text":"","title":"Problem Statement"},{"location":"static-web-on-aws/#techonology-stack","text":"","title":"Techonology Stack"},{"location":"static-web-on-aws/#architecture-diagram","text":"","title":"Architecture Diagram"},{"location":"static-web-on-aws/#project-requirements","text":"Your team has asked you to create a way to automate the deployment of a website. Currently your developers have to go through the process manually to test each new update to their code. You\u2019ll need to provide the static site URL to the developers and also make a modification to the code in the GitHub repo to verify the pipeline is working. Create a new repository in GitHub or CodeCommit and load the attached HTML. Create and configure a S3 bucket to host your static website. Create a CI/CD pipeline using the AWS Codepipeline service . Set your repo as the Source Stage of the Codepipeline that is triggered when an update is made to a GitHub repo. For the deploy stage select your S3 bucket. Deploy the pipeline and verify that you can reach the static website. Make an update to the code in your github to verify that the codepipeline is triggered. This can be as simple as a change to the Readme file because any change to the files should trigger the workflow. Note: you can skip the Build stage for this project. Your app is very popular all around the world but some users are complaining about slow load times in some Regions. You have been asked to add CloudFront as a CDN for your static website. CloudFront should allow caching of your static webpage and only allow HTTPS traffic to your site.","title":"Project Requirements:"},{"location":"static-web-on-aws/#instructions","text":"Create New Repository and Clone it. First we need to create a repository. Navigate to GitHub -> Repositories -> Create Repository and give it a name. Use the Clone URL to clone it to your local system. Add your files to your local repository, commit your changes, and push your changes. File has been pushed from our local repo to CodeCommit. Create S3 Bucket Navigate to S3 -> Create Bucket. Uncheck \u201cBlock all Public Access\u201d and acknowledge. Navigate to your bucket -> Properties -> Edit Static website hosting Enable Static website hosting and add your index document Now we need to create a bucket policy. Got to Permissions and edit the bucket policy. The following will allow everyone to access the bucket using the GetObject command, Setup Pipeline. Navigate to CodePipeline -> Create pipeline provide a name and click next. Source Provider = AWS CodeCommit Repository name = \u201cSelect your repo from the list\u201d Branch Name = Master","title":"Instructions"},{"location":"vpc-flow-logs-on-aws/","text":"AWS Vpc Flow Logs Coming Soon !","title":"AWS Vpc Flow Logs"},{"location":"vpc-flow-logs-on-aws/#aws-vpc-flow-logs","text":"","title":"AWS Vpc Flow Logs"},{"location":"vpc-flow-logs-on-aws/#coming-soon","text":"","title":"Coming Soon !"},{"location":"vpc-foundation-terraform-on-aws/","text":"Virtual Private Cloud Architecture on AWS using Terraform \ud83d\ude80 Overview: The VPC Architecture project on AWS using Terraform aims to create a scalable and resilient infrastructure that leverages the power of Amazon Web Services (AWS) cloud platform. This project utilizes Terraform, an Infrastructure as Code (IaC) tool, to provision and manage the infrastructure components, enabling automation, repeatability, and scalability. The primary objective of this project is to design and deploy a virtual Private Cloud or Networking architecture on AWS that consists of multiple compoments, including basement, networking, traffic flow. All compoments will be deployed across two Availability Zones (AZs) for high availability and fault tolerance. \ud83d\udd27 Problem Statement Terraform is an IaC software tool that provides a consistent command line interface (CLI) workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. In this specific case you need to create foundation Networking(VPC, Subnets, route table, IGW, NAT Gateway...), Terraform will automatically use the configuration files to provide the infrastructure resources where we can run application needed. Terraform will use his deployment to provide all AWS needed elements avoiding us to use the console and it will automate the setup, ensuring consistency and reducing human error. \ud83d\udcbd Techonology Stack The architecture consists of the following three tiers: VPC : AWS VPC Subnets : AWS Subnets Route table : AWS route table NACL : AWS NACL Internet Gateway : AWS IGW \ud83d\udccc Architecture Diagram \ud83c\udf1f Project Requirements Before you get started, make sure you have the following prerequisites in place: Terraform installed on your local machine. AWS IAM credentials configured in your text editor. In this case we will use VSCODE. Git installed on your local machine and Github account set up Github Git for cloning the repository. You must also know Terraform workflow \ud83d\udccb Table of Contents I - Terraform Configuration files Step 1: Provider Configuration Step 2: Variables Configuration Step 3: VPC Configuration Step 4: Output Configuration II - Instructions of Deployment Step 5: Clone Repository Step 6: Initialize Folder Step 7: Format Files Step 8: Validate Files Step 9: Plan Step 10: Apply Step 11: Review of Resources Step 12: Destroy \u2728Terraform Configuration files You need to write different files generating resources Step 1: Provider Configuration Here we declare our cloud provider and we specify the region where we will be launching resources provider Configuration Step 2: Variables Configuration This is where we declare all variables and thier value. It includes Variables : List of element that can vary or change. They can be reuse values throughout our code without repeating ourselves and help make the code dynamic values : values attributed to each variables. We have variables Configuration value Configuration Step 3: VPC Configuration This is where you create the basement, foundation and networking where all the resources will be launch. It includes VPC, Subnets, IGW, NatGateway, EIP and Route tables VPC Configuration Web Servers : These run your application code that contains the apache which will deploy the index.html located in the user data. Load Balancer : Distributes traffic across multiple web servers running in the public subnets. Auto Scaling : Automatically adjusts the number of web servers based on traffic. Security Groups : Controls incoming and outgoing traffic from outside to the web servers. Step 4: Output Configuration Know as Output Value : it is a convenient way to get useful information about your infranstructure printed on the CLI. It is showing the ARN, name or ID of a resource. In this case we are bringing out the DNS name of the web application Load balancer. Output Configuration \ud83d\udcbc Instructions of Deployment Follow these steps to deploy the architecture: Step 5: Clone Repository: Clone the repository in your local machine using the command \"git clone\" bash git clone https://github.com/cloudspaceacademy/terraform-on-aws.git Step 6: Initialize Folder Initialize the folder containing configuation files that were clone to Terraform and apply the configuration by typing the following command bash terraform init You must see this image Step 7: Format Files Apply any changes on files and Review the changes and confirm the good format with command: bash terraform fmt Step 8: Validate Files Ensure that every files are syntactically valid and ready to go with the command: bash terraform validate If everything is good you will have something like this Step 9: Plan Create an excution plan to provide the achievement of the desired state. It Check and confirm the numbers of resources that will be create. Use command: bash terraform plan The list of all resources in stage of creation will appear and you can see all properties(arguments and attributs) of each resouces Step 10: Apply Bring all desired state resources on life. It Launch and create all resources listed in the configuration files. The command to perform the task is: shell terraform apply -auto-approve You will be prompt to type the username and password for the database. After you enter those criticals data the process of creation will start and you will be able to see which resourse is on the way to be create and the time it taking to create. At the end you will recieve a prompt message showing all resources status: created, changed and the numbers of them. Step 11: Review of resources Go back on the console and check all actual state resources one by one to see. You will have VPC Subnets IGW Route Tables NCAL Step 12: Destroy Destroy the terraform managed infrastructure meaning all resourcescreated will be shut down. This action can be done with the command \"terraform destroy\" bash terraform destroy -auto-approve At the end you will recieve a prompt message showing all resources has been destroyed \ud83d\udcc4 License This project is licensed under the Joebaho Cloud License","title":"VPC Foundation on AWS"},{"location":"vpc-foundation-terraform-on-aws/#virtual-private-cloud-architecture-on-aws-using-terraform","text":"","title":"Virtual Private Cloud Architecture on AWS using Terraform"},{"location":"vpc-foundation-terraform-on-aws/#overview","text":"The VPC Architecture project on AWS using Terraform aims to create a scalable and resilient infrastructure that leverages the power of Amazon Web Services (AWS) cloud platform. This project utilizes Terraform, an Infrastructure as Code (IaC) tool, to provision and manage the infrastructure components, enabling automation, repeatability, and scalability. The primary objective of this project is to design and deploy a virtual Private Cloud or Networking architecture on AWS that consists of multiple compoments, including basement, networking, traffic flow. All compoments will be deployed across two Availability Zones (AZs) for high availability and fault tolerance.","title":"\ud83d\ude80 Overview:"},{"location":"vpc-foundation-terraform-on-aws/#problem-statement","text":"Terraform is an IaC software tool that provides a consistent command line interface (CLI) workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. In this specific case you need to create foundation Networking(VPC, Subnets, route table, IGW, NAT Gateway...), Terraform will automatically use the configuration files to provide the infrastructure resources where we can run application needed. Terraform will use his deployment to provide all AWS needed elements avoiding us to use the console and it will automate the setup, ensuring consistency and reducing human error.","title":"\ud83d\udd27 Problem Statement"},{"location":"vpc-foundation-terraform-on-aws/#techonology-stack","text":"The architecture consists of the following three tiers: VPC : AWS VPC Subnets : AWS Subnets Route table : AWS route table NACL : AWS NACL Internet Gateway : AWS IGW","title":"\ud83d\udcbd Techonology Stack"},{"location":"vpc-foundation-terraform-on-aws/#architecture-diagram","text":"","title":"\ud83d\udccc Architecture Diagram"},{"location":"vpc-foundation-terraform-on-aws/#project-requirements","text":"Before you get started, make sure you have the following prerequisites in place: Terraform installed on your local machine. AWS IAM credentials configured in your text editor. In this case we will use VSCODE. Git installed on your local machine and Github account set up Github Git for cloning the repository. You must also know Terraform workflow","title":"\ud83c\udf1f Project Requirements"},{"location":"vpc-foundation-terraform-on-aws/#table-of-contents","text":"I - Terraform Configuration files Step 1: Provider Configuration Step 2: Variables Configuration Step 3: VPC Configuration Step 4: Output Configuration II - Instructions of Deployment Step 5: Clone Repository Step 6: Initialize Folder Step 7: Format Files Step 8: Validate Files Step 9: Plan Step 10: Apply Step 11: Review of Resources Step 12: Destroy","title":"\ud83d\udccb Table of Contents"},{"location":"vpc-foundation-terraform-on-aws/#terraform-configuration-files","text":"You need to write different files generating resources Step 1: Provider Configuration Here we declare our cloud provider and we specify the region where we will be launching resources provider Configuration Step 2: Variables Configuration This is where we declare all variables and thier value. It includes Variables : List of element that can vary or change. They can be reuse values throughout our code without repeating ourselves and help make the code dynamic values : values attributed to each variables. We have variables Configuration value Configuration Step 3: VPC Configuration This is where you create the basement, foundation and networking where all the resources will be launch. It includes VPC, Subnets, IGW, NatGateway, EIP and Route tables VPC Configuration Web Servers : These run your application code that contains the apache which will deploy the index.html located in the user data. Load Balancer : Distributes traffic across multiple web servers running in the public subnets. Auto Scaling : Automatically adjusts the number of web servers based on traffic. Security Groups : Controls incoming and outgoing traffic from outside to the web servers. Step 4: Output Configuration Know as Output Value : it is a convenient way to get useful information about your infranstructure printed on the CLI. It is showing the ARN, name or ID of a resource. In this case we are bringing out the DNS name of the web application Load balancer. Output Configuration","title":"\u2728Terraform Configuration files"},{"location":"vpc-foundation-terraform-on-aws/#instructions-of-deployment","text":"Follow these steps to deploy the architecture: Step 5: Clone Repository: Clone the repository in your local machine using the command \"git clone\" bash git clone https://github.com/cloudspaceacademy/terraform-on-aws.git Step 6: Initialize Folder Initialize the folder containing configuation files that were clone to Terraform and apply the configuration by typing the following command bash terraform init You must see this image Step 7: Format Files Apply any changes on files and Review the changes and confirm the good format with command: bash terraform fmt Step 8: Validate Files Ensure that every files are syntactically valid and ready to go with the command: bash terraform validate If everything is good you will have something like this Step 9: Plan Create an excution plan to provide the achievement of the desired state. It Check and confirm the numbers of resources that will be create. Use command: bash terraform plan The list of all resources in stage of creation will appear and you can see all properties(arguments and attributs) of each resouces Step 10: Apply Bring all desired state resources on life. It Launch and create all resources listed in the configuration files. The command to perform the task is: shell terraform apply -auto-approve You will be prompt to type the username and password for the database. After you enter those criticals data the process of creation will start and you will be able to see which resourse is on the way to be create and the time it taking to create. At the end you will recieve a prompt message showing all resources status: created, changed and the numbers of them. Step 11: Review of resources Go back on the console and check all actual state resources one by one to see. You will have VPC Subnets IGW Route Tables NCAL Step 12: Destroy Destroy the terraform managed infrastructure meaning all resourcescreated will be shut down. This action can be done with the command \"terraform destroy\" bash terraform destroy -auto-approve At the end you will recieve a prompt message showing all resources has been destroyed","title":"\ud83d\udcbc Instructions of Deployment"},{"location":"vpc-foundation-terraform-on-aws/#license","text":"This project is licensed under the Joebaho Cloud License","title":"\ud83d\udcc4 License"}]}